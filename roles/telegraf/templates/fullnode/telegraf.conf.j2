[agent]
  hostname = "{{ org }}-{{ node_service_identifier }}-{{ env }}"
  flush_interval = "30s"
  interval = "30s"
  collection_jitter = "5s"
  debug = true  

[global_tags]
  type = "{{ type }}"
  env = "{{ env }}"
  project = "supra"

# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = false
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  report_active = false

[[inputs.mem]]

[[inputs.system]]

[[inputs.disk]]
  mount_points = ["/"]
[[inputs.diskio]]
[[inputs.processes]]
# Sysstat metrics collector
# Sysstat metrics collector
# This plugin ONLY supports Linux
[[inputs.sysstat]]
  ## Path to the sadc command.
  #
  ## Common Defaults:
  ##   Debian/Ubuntu: /usr/lib/sysstat/sadc
  ##   Arch:          /usr/lib/sa/sadc
  ##   RHEL/CentOS:   /usr/lib64/sa/sadc
  sadc_path = "/usr/lib/sysstat/sadc" # required

  ## Path to the sadf command, if it is not in PATH
  # sadf_path = "/usr/bin/sadf"

  ## Activities is a list of activities, that are passed as argument to the
  ## sadc collector utility (e.g: DISK, SNMP etc...)
  ## The more activities that are added, the more data is collected.
   activities = ["DISK"]

  ## Group metrics to measurements.
  ##
  ## If group is false each metric will be prefixed with a description
  ## and represents itself a measurement.
  ##
  ## If Group is true, corresponding metrics are grouped to a single measurement.
  # group = true

  ## Options for the sadf command. The values on the left represent the sadf options and
  ## the values on the right their description (which are used for grouping and prefixing metrics).
  ##
  ## Run 'sar -h' or 'man sar' to find out the supported options for your sysstat version.
  [inputs.sysstat.options]
    -C = "cpu"
    -B = "paging"
    -b = "io"
    -d = "disk"             # requires DISK activity
    "-n ALL" = "network"
    "-P ALL" = "per_cpu"
    -q = "queue"
#    -R = "mem"
    -r = "mem_util"
    -S = "swap_util"
    -u = "cpu_util"
    -v = "inode"
    -W = "swap"
    -w = "task"
  # -H = "hugepages"        # only available for newer linux distributions
  # "-I ALL" = "interrupts" # requires INT activity

  ## Device tags can be used to add additional tags for devices. For example the configuration below
  ## adds a tag vg with value rootvg for all metrics with sda devices.
  # [[inputs.sysstat.device_tags.sda]]
  #  vg = "rootvg"
[[inputs.procstat]]
  pattern = ".*"
[[inputs.nstat]]
[[inputs.net]]

[[inputs.exec]]
  commands = ["/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/check_vals_rpc_ports.sh | bash -s {{ supra.fullnode.ports.p2p_port }}'"]
  timeout = "5s"
  data_format = "influx"

[[inputs.exec]]
  commands = ["/bin/bash -c 'echo {{ supra.fullnode.version }}'"]
  timeout = "5s"
  data_format = "value"
  data_type = "string"
  name_override = "supra_version"

[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/total_transaction_rpc.sh | bash -s {{ telegraf_supra_log_file_path }}'"
  timeout = "600s"
  data_format = "influx"
 # data_type = "string"
  name_override = "total_transactions" 
  interval = "300s"


[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/epoch-rpc.py | python3 - {{ telegraf_supra_log_file_path }}'"
  timeout = "600s"
  data_format = "influx"
  name_override = "epoch_metrics" 
  interval = "300s"

[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/vals_participate_rpc.sh | bash -s {{ telegraf_supra_log_file_path }}'"
  timeout = "600s"
  data_format = "value"
  data_type ="string"
  name_override = "count_vals"
  interval = "300s"


[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/block_rate_rpc.py | python3 - {{ telegraf_supra_log_file_path }}'"
  timeout = "15s"
  data_format = "influx"
  name_override = "block_rate"
 

[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/vals_count_rpc.sh | bash -s {{ telegraf_supra_log_file_path }}'"
  timeout = "600s"
  data_format = "value"
  data_type = "integer"
  name_override = "vals_participated" 
  interval = "300s"
  
[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/sync_rpc.py | python3 - {{ telegraf_supra_log_file_path }}'"
  data_format = "influx"
  name_override = "sync_status"
  interval = "300s"

[[inputs.file]]
  files = ["/tmp/geo.json"]
  data_format = "json"                
  json_string_fields = [] 
  name_override = "node_geolocation"      

[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/blocks_rpc.py | python3 - {{ telegraf_supra_log_file_path }}'"
  timeout = "15s"
  data_format = "influx"
  name_override = "block_values"

[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/txn-metrics-rpc.py | python3 - {{ telegraf_supra_log_file_path }}'"
  timeout = "15s"
  data_format = "influx"
  name_override = "metrics_txn"
  
[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/master_dash_rpc.py | python3 - {{ telegraf_supra_log_file_path }}'"
  data_format = "influx"
  name_override = "validator_metrics"
  # interval = "300s"
  
[[inputs.exec]]
  command = "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/mainnet_validator_rpc.py | python3 - {{ telegraf_supra_log_file_path }}'"
  data_format = "influx"
  name_override = "mainnet_validator"
  # interval = "300s"
  timeout = "60s"

[[inputs.exec]]
  "/bin/bash -c 'curl -s https://raw.githubusercontent.com/encapsulate-xyz/supra-node-monitoring-tool/refs/heads/master/metrics-scripts/encapsulate_metrics_script.py | python3 - {{ type }} {{ env }}'"
  data_format = "prometheus"
  timeout = "60s"

[[outputs.influxdb_v2]]
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
  urls = ["http://34.100.248.227:8086"]

  ## Token for authentication.
  token = "{{ vault.supra.external.telegraf_token }}"
  timeout = "15s"
  ## Organization is the name of the organization you wish to write to; must exist.
  organization = "Entropy Foundation"

  ## Destination bucket to write into.
  bucket = "supra-metrics" 

[[outputs.influxdb]]
  urls = ["http://{{ monitor_server_dns }}:9122"]
  namepass = ["prometheus"]
